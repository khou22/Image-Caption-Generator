{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/kevinhou/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pickle import load\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from utils.model import CNNModel, generate_caption_beam_search\n",
    "import os\n",
    "from utils.timer import Timer\n",
    "from config import config\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    *Some simple checking\n",
    "\"\"\"\n",
    "assert type(config['max_length']) is int, 'Please provide an integer value for `max_length` parameter in config.py file'\n",
    "assert type(config['beam_search_k']) is int, 'Please provide an integer value for `beam_search_k` parameter in config.py file'\n",
    "\n",
    "# Extract features from each image in the directory\n",
    "def extract_features(filename, model, model_type):\n",
    "    if model_type == 'inceptionv3':\n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "        target_size = (299, 299)\n",
    "    elif model_type == 'vgg16':\n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "        target_size = (224, 224)\n",
    "    # Loading and resizing image\n",
    "    image = load_img(filename, target_size=target_size)\n",
    "    # Convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # Reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # Prepare the image for the CNN Model model\n",
    "    image = preprocess_input(image)\n",
    "    # Pass image into model to get encoded features\n",
    "    features = model.predict(image, verbose=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer_path = config['tokenizer_path']\n",
    "tokenizer = load(open(tokenizer_path, 'rb'))\n",
    "\n",
    "# Max sequence length (from training)\n",
    "max_length = config['max_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        'name': 'VGG16',\n",
    "        'load_path': 'model_data/model_vgg16_epoch-11_train_loss-2.2639_val_loss-3.1549.hdf5',\n",
    "        'model_type': 'vgg16'\n",
    "    },\n",
    "    {\n",
    "        'name': 'InceptionV3 Beam 3',\n",
    "        'load_path': 'model_data/model_inceptionv3_epoch-20_train_loss-2.3803_val_loss-2.8221-beam3.hdf5',\n",
    "        'model_type': 'inceptionv3'\n",
    "    },\n",
    "    {\n",
    "        'name': 'InceptionV3 Beam 5',\n",
    "        'load_path': 'model_data/model_inceptionv3_epoch-19_train_loss-2.3802_val_loss-2.8636-beam5.hdf5',\n",
    "        'model_type': 'inceptionv3'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVGG16: 9 seconds\n",
      "WARNING:tensorflow:From /Users/kevinhou/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "\tInceptionV3 Beam 3: 31 seconds\n",
      "\tInceptionV3 Beam 5: 43 seconds\n",
      "Total Model Load Time: 43 seconds\n"
     ]
    }
   ],
   "source": [
    "timer.start('Model Load')\n",
    "\n",
    "image_models = {}\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_name = model['name']\n",
    "    model_type = model['model_type']\n",
    "    \n",
    "    timer.start(model_name)\n",
    "    \n",
    "    # Load the caption model and image model\n",
    "    caption_model = load_model(model['load_path'])\n",
    "    \n",
    "    if model_type not in image_models:\n",
    "        image_models[model_type] = CNNModel(model_type)\n",
    "    \n",
    "    # Set properties back on model object\n",
    "    models[i]['model'] = caption_model\n",
    "    \n",
    "    print(\"\\t%s: %d seconds\" % (model_name, timer.end('Model Load')))\n",
    "\n",
    "print(\"Total Model Load Time: %d seconds\" % timer.end('Model Load'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = config['test_data_path'] + 'people-working.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START]\n",
      "\n",
      "VGG16\n",
      "\tCaption: woman is sitting at a table at a restaurant\n",
      "\tDuration: 2 seconds\n",
      "\n",
      "InceptionV3 Beam 3\n",
      "\tCaption: group of people sit in front of a restaurant\n",
      "\tDuration: 4 seconds\n",
      "\n",
      "InceptionV3 Beam 5\n",
      "\tCaption: group of people are standing in front of a glass fountain\n",
      "\tDuration: 4 seconds\n",
      "\n",
      "[END] Image Analysis Time: 10\n"
     ]
    }
   ],
   "source": [
    "timer.start('Image Analysis')\n",
    "print(\"[START]\")\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model_name = model['name']\n",
    "    model_type = model['model_type']\n",
    "    image_model = image_models[model_type]\n",
    "    caption_model = model['model']\n",
    "    \n",
    "    timer.start(model_name)\n",
    "    \n",
    "    image_features = extract_features(image_path, image_model, model_type)\n",
    "    generated_caption_seq = generate_caption_beam_search(caption_model, tokenizer, image_features, max_length, beam_index=config['beam_search_k'])\n",
    "    generated_caption = ' '.join(generated_caption_seq.split()[2:len(generated_caption_seq.split())-1])\n",
    "    \n",
    "    models[i]['caption'] = generated_caption\n",
    "    \n",
    "    print(\"\\n%s\" % model_name)\n",
    "    print(\"\\tCaption: %s\" % generated_caption)\n",
    "    print(\"\\tDuration: %d seconds\" % timer.end(model_name))\n",
    "\n",
    "print(\"\\n[END] Image Analysis Time: %d\" % timer.end('Image Analysis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [x['caption'] for x in models]\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
